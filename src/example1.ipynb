{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study\n",
    "## Task\n",
    "Predict income and expenses for a holdout sample of ~10K users for the month of August based on a training sample of ~10K users from Februrary through July.\n",
    "\n",
    "Based on your judgement of the usefulness of the results, either aggregate the data into incoming & outgoing flows, or predict based on the transaction type/category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# custom modules\n",
    "from LRmodel import LogisticRegressionModels\n",
    "import plotter\n",
    "from preprocessing import load_and_preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data\n",
    "'''\n",
    "# data_train = pd.read_csv('./data/2016-09-19_79351_training.csv')\n",
    "data_train = pd.read_csv(\"./data/2016-09-19_79351_training_feb_june.csv\")\n",
    "data_mcc_group = pd.read_csv('./data/mcc_group_definition.csv')\n",
    "data_transaction = pd.read_csv('./data/transaction_types.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_main = data_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_main.dataset_transaction.unique(), data_train_main.dataset_user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_main.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features `dataset_transaction` and  `dataset_user` contains only one unique entry which are irrelevant. We simply drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_main = data_train_main.drop(['dataset_transaction', 'dataset_user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_main.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    " - create ```direction``` (i.e. in/out) feature: the ```in``` and ```out``` represents the income and expenses, respectively. ```in``` denotes money coming into the user acccount and ```out``` denotes money going out of the user account.\n",
    "\n",
    " - creating month/day feature from the transaction date.\n",
    "\n",
    " - fill NAs in ```mcc_group``` with mode and miscellaneous. We will fill the NAs locally per user. Specifically, we will take the ```mcc_group``` of a user and check for the most frequent transaction type and use this mode to replace any NA for such user. In the event there is no mode, we will simply categorize this as miscellaneous (i.e. 16) since no modal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "filtered data_transaction based in transaction_type\n",
    "'''\n",
    "\n",
    "# filtering data_transaction.type\n",
    "\n",
    "# data_train_main.user_id.nunique()\n",
    "selected_types = list(data_train_main.transaction_type.unique()); selected_types\n",
    "data_transaction_filtered = data_transaction[data_transaction.type.isin(selected_types)]\n",
    "\n",
    "data_transaction_filtered.reset_index(drop = True, inplace = True); data_transaction_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the transaction type into in and out\n",
    "type_in = data_transaction_filtered[data_transaction_filtered.direction == 'In'].type\n",
    "type_out = data_transaction_filtered[data_transaction_filtered.direction == 'Out'].type\n",
    "type_in, type_out = list(type_in), list(type_out); \n",
    "type_in, type_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding in/out feature to data_train_main\n",
    "\n",
    "data_train_main['direction'] = data_train_main.transaction_type.apply(lambda x: 'In' if x in type_in else 'Out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping and adding agent feature to data_train_main\n",
    "\n",
    "type_to_agent = {\n",
    "    'BBU': 'Partner',\n",
    "    'CT': 'Bank Account',\n",
    "    'DR': 'Bank Account',\n",
    "    'PF': 'Card',\n",
    "    'PT': 'Card',\n",
    "    'BUB': 'Partner',\n",
    "    'DD': 'Bank Account',\n",
    "    'DT': 'Bank Account',\n",
    "    'FT': 'Bank Account',\n",
    "    'TUB': 'Partner'\n",
    "}\n",
    "\n",
    "data_train_main['agent'] = data_train_main.transaction_type.apply(lambda x: type_to_agent[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick check on the data_train_main\n",
    "data_train_main[data_train_main.transaction_type == 'PT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract day/month from data_train_main.transaction_date\n",
    "data_train_main['transaction_date'] = pd.to_datetime(data_train_main.transaction_date)\n",
    "data_train_main['day'] = data_train_main.transaction_date.dt.day\n",
    "data_train_main['month'] = data_train_main.transaction_date.dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in the missing values in mcc_group feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/na_fill.png)\n",
    "\n",
    "\n",
    "As shown above, we filled the NAs locally per user. Specifically, we take the ```mcc_group``` of a user and check for the most frequent transaction type and use this mode to replace any NA for such user. In the event there is no mode, we simply categorize this as miscellaneous (i.e. 16) since no modal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill missing values with mode or miscellaneous\n",
    "def fill_missing_with_mode(df, group_col, target_col):\n",
    "    \n",
    "    def mode_function(x):\n",
    "        modes = x.mode()\n",
    "        if not modes.empty:\n",
    "            return modes[0]\n",
    "        else:\n",
    "            return 16 #miscellaneous\n",
    "    \n",
    "    df[target_col] = df.groupby(group_col)[target_col].transform(lambda x: x.fillna(mode_function(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "data_train_main_filled = fill_missing_with_mode(data_train_main, 'user_id', 'mcc_group')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the result for a user\n",
    "[data_train_main_filled['user_id']=='001f9baedaf3c8487c344d25b0eda9fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of mcc_group before filling NAs\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "data_train_main.mcc_group.value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of mcc_group after fill\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "data_train_main_filled.mcc_group.value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking distribution of response feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of direction\n",
    "data_train_main_filled.direction.value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that the target variable `direction` is imbalanced. With income accounting for 18% and expenses 82%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage distribution of direction\n",
    "data_train_main_filled.direction.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical columns\n",
    "\n",
    "categorical_cols = data_train_main_filled.select_dtypes(include = 'object').columns\n",
    "print(f'categorical_cols:{categorical_cols}')\n",
    "for col in categorical_cols[1:]:\n",
    "    data_train_main_filled[col] = LabelEncoder().fit_transform(data_train_main_filled[col])\n",
    "\n",
    "\n",
    "\n",
    "# set mcc_group into integer\n",
    "\n",
    "data_train_main_filled['mcc_group'] = data_train_main_filled['mcc_group'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A view of the updated data\n",
    "data_train_main_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping transaction_date, transaction_type, user_id\n",
    "data_train_main_filled.drop(['user_id', 'transaction_date', 'transaction_type'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick correlation check\n",
    "data_train_main_filled.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split data\n",
    "'''\n",
    "X = data_train_main_filled.drop('direction', axis=1)\n",
    "y = data_train_main_filled['direction']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the proportion of the classes in the train, validation and test sets\n",
    "y_train.value_counts(normalize=True) * 100, y_val.value_counts(normalize=True) * 100, y_test.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "grids search on RF\n",
    "'''\n",
    "\n",
    "run_grid_search = False\n",
    "if run_grid_search:\n",
    "    \n",
    "    param_grid = {\n",
    "        'class_weight': [{0: 1.3, 1: 1.0},{0: 1.4, 1: 1.0}, {0: 1.45, 1: 1.0}, {0: 1.5, 1: 1.0}],\n",
    "        # 'class_weight': [{0: 2.8, 1: 0.6}, {0: 2.4, 1: 0.6},  {0: 2.2, 1: 0.6}],\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [None, 10, 20],\n",
    "    }\n",
    "    \n",
    "\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, scoring='roc_auc', cv=3, verbose=3)\n",
    "   \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_;best_params\n",
    "    print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "![title](./img/lr.png)\n",
    "## 1. LR (logistic regression): \n",
    "we use the LR methods as baseline and in addition, we add penalties to the cost function to balance the target variable.\n",
    "\n",
    "## 2. RF (random forest)\n",
    "\n",
    "![title](img/rfTree.png)\n",
    "`(image source: https://upload.wikimedia.org/wikipedia/commons/4/4e/Random_forest_explain.png)`\n",
    "\n",
    "In classification problem, the RF method works by constructing multiple decision trees during training and returns the majority vote from these trees.\n",
    "\n",
    "baseline RF assumes equal importance for all classes, and the weighted approach introduces class weights to modify the splitting criteria of the labels to prioritize minority class.\n",
    "\n",
    "## 3. MLP (multilayer perceptron)\n",
    "mlp is a feedforward neural network consisting of fully connected neurons with nonlinear activation functions \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paramter:\n",
    "\n",
    "- `class_weight`: in the weighted RF, changes to `class_weight` will modify the outcome of the model. after some experiments, the class weight is set as `class_weight={0:1.45, 1:1.0}`\n",
    "- `max_depth`: the maximum depth of the tree\n",
    "- `n_estimator`: higher (lower) `n_estimator` could increase (reduce) model accuracy\n",
    "- weights on LR: to tune the model sensitivity to imbalanced response feature, use `weight_positive=1.` and  `weight_negative=1.45`. these values are thesame as in `class_weight`\n",
    "- `hidden_layer_sizes`: The ith element represents the number of neurons in the ith hidden layer.\n",
    "- `activation`: activation function for the hidden layer. other options are {identity, logistic, tanh}.\n",
    "- `solver`: the solver for weight optimization. other options to try are `lbfgs` and `sgd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train model\n",
    "'''\n",
    "\n",
    "n_estimator = 100\n",
    "seedS = 42\n",
    "lr = 0.01\n",
    "max_iter = 1000\n",
    "max_depth = 15\n",
    "\n",
    "models = {\n",
    "    \"Baseline RF\": RandomForestClassifier(n_estimators=n_estimator, random_state=seedS),\n",
    "    \"Weighted RF\": RandomForestClassifier(n_estimators=n_estimator, class_weight={0:1.45, 1:1.0}, max_depth=max_depth, random_state=seedS),\n",
    "    \"Baseline LR\": LogisticRegressionModels(model_type=\"baseline\", learning_rate=lr, max_iter=max_iter),\n",
    "    \"cost LR\": LogisticRegressionModels(model_type=\"cost_sensitive\", weight_positive=1., weight_negative=1.45, learning_rate=lr, max_iter=max_iter),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', verbose=False,learning_rate_init=lr, max_iter=max_iter, random_state=seedS),\n",
    "} #relu\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}\")\n",
    "    if model.__class__.__name__ in ['RandomForestClassifier', 'MLPClassifier']:\n",
    "        model.fit(X_train, y_train)\n",
    "    elif model.__class__.__name__ == 'LogisticRegressionModels':\n",
    "        model.fit(X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        raise ValueError(\"Model not supported\")\n",
    "        \n",
    "    show_curves = False\n",
    "    if show_curves:\n",
    "        if isinstance(model, (LogisticRegressionModels)):  \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(model.train_losses, label='Training Loss')\n",
    "            plt.plot(model.val_losses, label='Validation Loss')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'{name} Training and Validation Loss Curves')\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "    # pred\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "\n",
    "    # metric\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    if model.__class__.__name__ in ['RandomForestClassifier', 'MLPClassifier']:\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "    elif model.__class__.__name__ == 'LogisticRegressionModels':\n",
    "        y_pred_proba_test = model.predict_proba(X_test)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "    # Save model\n",
    "     # joblib.dump(model, f\"models/{name.replace(' ', '_')}_model.pkl\")\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Val Accuracy\": val_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"AUC\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"Model\": model,\n",
    "        }\n",
    "    print(f\"--- {name} saved ---\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"Val Accuracy: {val_accuracy:.2f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check test accuracy\n",
    "methods = list(results.keys())\n",
    "test_accuracies_ = [metrics['Test Accuracy'] for metrics in results.values()]\n",
    "\n",
    "list(zip(methods,test_accuracies_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot accuraries\n",
    "'''\n",
    "\n",
    "labels = list(results.keys())\n",
    "train_accuracies = [metrics['Train Accuracy'] for metrics in results.values()]\n",
    "val_accuracies = [metrics['Val Accuracy'] for metrics in results.values()]\n",
    "test_accuracies = [metrics['Test Accuracy'] for metrics in results.values()]\n",
    "\n",
    "bar_width = 0.2\n",
    "indices = np.arange(len(labels))\n",
    "figsize = (16, 8)\n",
    "plotter.plot_barplot( indices, train_accuracies, val_accuracies, test_accuracies, labels, bar_width=bar_width, figsize=figsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the F1 score\n",
    "f1_ = [metrics['f1'] for metrics in results.values()]\n",
    "\n",
    "list(zip(methods,f1_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot AUC curves\n",
    "'''\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_auc_curves(results, X_test, y_test, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    ROC curves here.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for name, metrics in results.items():\n",
    "        model = metrics[\"Model\"]\n",
    "        if model.__class__.__name__ in ['RandomForestClassifier', 'MLPClassifier']:\n",
    "            y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba_test)\n",
    "        elif model.__class__.__name__ == 'LogisticRegressionModels':\n",
    "            y_pred_proba_test = model.predict_proba(X_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba_test)\n",
    "        else:\n",
    "            continue \n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {metrics[\"AUC\"]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guessing\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "figsize=(12, 8)\n",
    "plotter.plot_auc_curves(results, X_test, y_test, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from preprocessing import load_and_preprocess\n",
    "import preprocessing2 #import load_and_preprocessT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets compare `July_data` with `predict_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline_LR_prediction</th>\n",
       "      <th>Baseline_LR_meaning</th>\n",
       "      <th>Baseline_LR_probability</th>\n",
       "      <th>Baseline_RF_prediction</th>\n",
       "      <th>Baseline_RF_meaning</th>\n",
       "      <th>Baseline_RF_probability</th>\n",
       "      <th>Weighted_RF_prediction</th>\n",
       "      <th>Weighted_RF_meaning</th>\n",
       "      <th>Weighted_RF_probability</th>\n",
       "      <th>Cost-sensitive_LR_prediction</th>\n",
       "      <th>Cost-sensitive_LR_meaning</th>\n",
       "      <th>Cost-sensitive_LR_probability</th>\n",
       "      <th>MLP_prediction</th>\n",
       "      <th>MLP_meaning</th>\n",
       "      <th>MLP_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.416394e-43</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.3725</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.341104</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.244572e-49</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.459896e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>2.185507e-37</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.171639</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.373175e-42</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>5.842439e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>3.889676e-75</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.217397</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.504343e-85</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.586080e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>2.227612e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.3725</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.341104</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>5.113552e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.628183e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>1.405608e-02</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>0.217397</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>5.827044e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>In</td>\n",
       "      <td>9.510073e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Baseline_LR_prediction Baseline_LR_meaning  Baseline_LR_probability  \\\n",
       "0                       0                  In             1.416394e-43   \n",
       "1                       0                  In             2.185507e-37   \n",
       "2                       0                  In             3.889676e-75   \n",
       "3                       0                  In             2.227612e-04   \n",
       "4                       0                  In             1.405608e-02   \n",
       "\n",
       "   Baseline_RF_prediction Baseline_RF_meaning  Baseline_RF_probability  \\\n",
       "0                       0                  In                   0.3725   \n",
       "1                       0                  In                   0.1600   \n",
       "2                       0                  In                   0.1400   \n",
       "3                       0                  In                   0.3725   \n",
       "4                       0                  In                   0.1400   \n",
       "\n",
       "   Weighted_RF_prediction Weighted_RF_meaning  Weighted_RF_probability  \\\n",
       "0                       0                  In                 0.341104   \n",
       "1                       0                  In                 0.171639   \n",
       "2                       0                  In                 0.217397   \n",
       "3                       0                  In                 0.341104   \n",
       "4                       0                  In                 0.217397   \n",
       "\n",
       "   Cost-sensitive_LR_prediction Cost-sensitive_LR_meaning  \\\n",
       "0                             0                        In   \n",
       "1                             0                        In   \n",
       "2                             0                        In   \n",
       "3                             0                        In   \n",
       "4                             0                        In   \n",
       "\n",
       "   Cost-sensitive_LR_probability  MLP_prediction MLP_meaning  MLP_probability  \n",
       "0                   1.244572e-49               0          In     1.459896e-07  \n",
       "1                   1.373175e-42               0          In     5.842439e-07  \n",
       "2                   1.504343e-85               0          In     1.586080e-10  \n",
       "3                   5.113552e-05               0          In     1.628183e-02  \n",
       "4                   5.827044e-03               0          In     9.510073e-02  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicted Data\n",
    "output_csv_path = \"./result/predictions_all_models.csv\"\n",
    "prediction_df_ = pd.read_csv(output_csv_path)\n",
    "prediction_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing2' from '/Users/abdullahi/Documents/github/income/src/preprocessing2.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original df shape: (70986, 7)\n",
      "actual False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdullahi/Documents/github/income/src/preprocessing2.py:56: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_train[\"transaction_date\"] = pd.to_datetime(df_train[\"transaction_date\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cols:Index(['direction', 'agent'], dtype='object')\n",
      "y:<class 'pandas.core.series.Series'>\n",
      "using --training == predict--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcc_group</th>\n",
       "      <th>amount_n26_currency</th>\n",
       "      <th>direction</th>\n",
       "      <th>agent</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcc_group  amount_n26_currency  direction  agent  day  month\n",
       "0          2                  202          1      0    7      1\n",
       "1          1                  173          1      0    7      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# July data\n",
    "new_data_dir = \"../data/2016-09-19_79351_training_july.csv\" \n",
    "new_data = preprocessing2.load_and_preprocessT(new_data_dir, training=False)\n",
    "\n",
    "new_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abdulGeneral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
